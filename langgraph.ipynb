{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoFsZcLyP7U/W+tQgexWWZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rakshit9/Generative-AI/blob/main/langgraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrWd6fFlwgZt"
      },
      "outputs": [],
      "source": [
        "!pip -q install \"langchain>=0.3.0\" \"langgraph>=0.2.0\" \"langchain-openai>=0.2.0\" \"python-dotenv>=1.0.1\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "if \"OPENAI_API_KEY\" not in os.environ or not os.environ[\"OPENAI_API_KEY\"]:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "print(\"API key set âœ…\")"
      ],
      "metadata": {
        "id": "nwKjarXjwomq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, List\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "from langgraph.graph import StateGraph, END\n"
      ],
      "metadata": {
        "id": "jVgqNDZKwwsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Define the state carried through the graph\n",
        "class GraphState(TypedDict, total=False):\n",
        "    messages: List           # chat history (HumanMessage/AIMessage objects)\n",
        "    thought: str             # model's reasoning text\n",
        "    answer: str              # final concise answer"
      ],
      "metadata": {
        "id": "SYIKp3TZw_-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Define your model (use a fast, cheap one)\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n"
      ],
      "metadata": {
        "id": "ghyaXlnnxElD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Define nodes (each node = one step)\n",
        "def think_node(state: GraphState) -> GraphState:\n",
        "    \"\"\"The agent thinks about the question.\"\"\"\n",
        "    # get the latest user message\n",
        "    user_msg = next((m for m in reversed(state.get(\"messages\", [])) if isinstance(m, HumanMessage)), None)\n",
        "    question = user_msg.content if user_msg else \"No question\"\n",
        "    response = llm.invoke([\n",
        "        SystemMessage(content=\"You are a helpful AI that explains reasoning clearly but concisely.\"),\n",
        "        HumanMessage(content=f\"Think step by step about this question, in 3â€“5 short bullets:\\n\\n{question}\")\n",
        "    ])\n",
        "    return {\n",
        "        \"thought\": response.content,\n",
        "        \"messages\": [AIMessage(content=f\"[Thought]\\n{response.content}\")]\n",
        "    }\n"
      ],
      "metadata": {
        "id": "0MFwMEK_xc7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_node(state: GraphState) -> GraphState:\n",
        "    \"\"\"The agent gives a clear final answer based on the thought.\"\"\"\n",
        "    thought = state.get(\"thought\", \"\")\n",
        "    response = llm.invoke([\n",
        "        SystemMessage(content=\"You are a friendly tutor. Be direct and concise.\"),\n",
        "        HumanMessage(content=f\"Based on this reasoning:\\n{thought}\\n\\nNow give a clear final answer in 3â€“5 sentences.\")\n",
        "    ])\n",
        "    return {\n",
        "        \"answer\": response.content,\n",
        "        \"messages\": [AIMessage(content=f\"[Answer]\\n{response.content}\")]\n",
        "    }"
      ],
      "metadata": {
        "id": "JU2BzzMaxeo8"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "think_node({\"messages\": [HumanMessage(content=\"What is the capital of France?\")]})"
      ],
      "metadata": {
        "id": "5C83FR0UzCm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_node({'thought': '- The question asks for the capital city of a specific country, France.\\n- The capital city is typically the political and administrative center of a country.\\n- France is a well-known country in Europe.\\n- The capital of France is Paris.\\n- Paris is recognized for its cultural, historical, and economic significance.',\n",
        " 'messages': [AIMessage(content='[Thought]\\n- The question asks for the capital city of a specific country, France.\\n- The capital city is typically the political and administrative center of a country.\\n- France is a well-known country in Europe.\\n- The capital of France is Paris.\\n- Paris is recognized for its cultural, historical, and economic significance.', additional_kwargs={}, response_metadata={})]})"
      ],
      "metadata": {
        "id": "GKvUjHhjzQr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 4) Build the graph (nodes + edges)\n",
        "graph = StateGraph(GraphState)\n",
        "graph.add_node(\"think\", think_node)\n",
        "graph.add_node(\"answer\", answer_node)"
      ],
      "metadata": {
        "id": "we0f6CXg0eh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "graph.set_entry_point(\"think\")      # START here\n",
        "graph.add_edge(\"think\", \"answer\")   # then go to answer\n",
        "graph.add_edge(\"answer\", END)       # then stop\n",
        "\n",
        "app = graph.compile()"
      ],
      "metadata": {
        "id": "iEW9UDpn0oTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "def run_agent(question: str):\n",
        "    initial_state: GraphState = {\"messages\": [HumanMessage(content=question)]}\n",
        "    result = app.invoke(initial_state)\n",
        "    print(\"ðŸ¤” Thought:\\n\", result.get(\"thought\", \"(no thought)\"), \"\\n\")\n",
        "    print(\"ðŸ’¬ Final Answer:\\n\", result.get(\"answer\", \"(no answer)\"))\n",
        "    return result\n",
        "\n",
        "\n",
        "print(\"Use run_agent('your question') to try it!\")"
      ],
      "metadata": {
        "id": "_Q9fsVED0p0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = run_agent(\"Why do we see lightning before we hear thunder?\")"
      ],
      "metadata": {
        "id": "uyZpOAaF0wH5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}